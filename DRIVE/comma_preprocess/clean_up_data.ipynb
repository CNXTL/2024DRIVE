{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import h5py\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "\n",
    "dataset_type = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_type == \"train\":\n",
    "            data_path = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_train_comma_chunk_1_w_imgs.hfd5\"\n",
    "            data_path2 = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_train_comma_chunk_2_w_imgs.hfd5\"\n",
    "            data_path3 = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_train_comma_chunk_3_w_imgs.hfd5\"\n",
    "elif dataset_type == \"test\":\n",
    "            data_path = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_test_comma_chunk_1_w_imgs.hfd5\"\n",
    "            data_path2 = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_test_comma_chunk_2_w_imgs.hfd5\"\n",
    "            data_path3 = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_test_comma_chunk_3_w_imgs.hfd5\"\n",
    "elif dataset_type == \"val\":\n",
    "            data_path = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_val_comma_chunk_1_w_imgs.hfd5\"\n",
    "            data_path2 = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_val_comma_chunk_2_w_imgs.hfd5\"\n",
    "            data_path3 = \"/hpc2hdd/home/tianlangxue/XAI4AD/concept_gridlock/comma_preprocess/gas_and_brake_val_comma_chunk_3_w_imgs.hfd5\"\n",
    "people_seqs = []\n",
    "\n",
    "h5_file = h5py.File(data_path, \"r\")\n",
    "keys = list(h5_file.keys())\n",
    "#keys.remove('10')\n",
    "#keys.remove('17')\n",
    "if dataset_type == \"train\":\n",
    "            #keys.remove('37')\n",
    "            #keys.remove('53')\n",
    "            #keys.remove('55')\n",
    "            #keys.remove('58')\n",
    "            h5_file2 = h5py.File(data_path2, \"r\")\n",
    "            keys2 = list(h5_file2.keys())\n",
    "            #good_keys = [0, 1, 4, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 19, 20, 21, 24, 25, 27, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 45, 46, 50, 52, 53]#[0, 1, 4, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 21, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 45, 46, 50, 52, 53]\n",
    "            #gk2 = np.array([55, 56, 57, 60, 62, 64, 65, 66, 67, 68, 69, 70, 72, 73, 76, 77, 78, 81, 82, 84, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106]) - 55# np.array([55, 56, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 81, 82, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106]) -55\n",
    "            keys = list(np.array(keys))#[good_keys])\n",
    "            keys2 = list(np.array(keys2))#[gk2])[:-1]\n",
    "            h5_file3 = h5py.File(data_path3, \"r\")\n",
    "            keys3 = list(h5_file3.keys())\n",
    "            #h5_file4 = h5py.File(data_path4, \"r\")\n",
    "            #keys4 = list(h5_file4.keys())\n",
    "#else:\n",
    "#            keys4 = []\n",
    "\n",
    "if dataset_type == \"val\":\n",
    "            #good_keys = [1, 6, 9, 10, 11, 12, 14, 15]\n",
    "            keys = np.array(keys)#[good_keys]\n",
    "            h5_file2 = h5py.File(data_path2, \"r\")\n",
    "            keys2 = list(h5_file2.keys())\n",
    "            h5_file3 = h5py.File(data_path3, \"r\")\n",
    "            keys3 = list(h5_file3.keys())\n",
    "            \n",
    "if dataset_type == \"test\":\n",
    "            #good_keys = [0, 6, 8, 9, 10, 13]\n",
    "            keys = np.array(keys)#[good_keys]\n",
    "            h5_file2 = h5py.File(data_path2, \"r\")\n",
    "            keys2 = list(h5_file2.keys())#[0:3] + list(h5_file2.keys())[5:]\n",
    "            h5_file3 = h5py.File(data_path3, \"r\")\n",
    "            keys3 = list(h5_file3.keys())#[:-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/data1/jessica/data/toyota/comma_train_filtered.h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h5py.File(f\"/hpc2hdd/home/tianlangxue/XAI4AD/comma2k19data/comma_{dataset_type}_w_desired_filtered.h5py\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:16<00:00,  8.52s/it]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(keys):\n",
    "    data = h5_file[key]\n",
    "    group  = h.create_group(key)\n",
    "    for col in data.keys():\n",
    "        dt = np.float32 if col != 'image' else int#\n",
    "        dataset_name = col #groups are divided by '/'\n",
    "        a = data[col]\n",
    "        group.create_dataset(dataset_name, data = a, compression_opts=6, compression='gzip', chunks=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:30<00:00,  9.04s/it]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(keys2):\n",
    "    data = h5_file2[key]\n",
    "    if key in h5_file2.keys():\n",
    "        key = key + \"_1\"\n",
    "    group  = h.create_group(key)\n",
    "    for col in data.keys():\n",
    "        #print(key,col)\n",
    "        dt = np.float32 if col != 'image' else int#\n",
    "        dataset_name = col #groups are divided by '/'\n",
    "        a = data[col]\n",
    "        group.create_dataset(dataset_name, data = a, compression_opts=6, compression='gzip', chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:09<00:00,  8.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(keys3):\n",
    "    data = h5_file3[key]\n",
    "    group  = h.create_group(key)\n",
    "    for col in data.keys():\n",
    "        dt = np.float32 if col != 'image' else int#\n",
    "        dataset_name = col #groups are divided by '/'\n",
    "        a = data[col]\n",
    "        group.create_dataset(dataset_name, data = a, compression_opts=6, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 27)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h.keys()), len(keys)+ len(keys2)+ len(keys3)\n",
    "# len(h.keys()), len(keys)+ len(keys2)+ len(keys3)+ len(keys4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 5/27 [00:00<00:01, 20.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020833333333333332\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.029166666666666667\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 12/27 [00:00<00:00, 21.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0375\n",
      "0.0\n",
      "0.09583333333333334\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 15/27 [00:00<00:00, 20.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15416666666666667\n",
      "0.0\n",
      "0.1375\n",
      "0.058333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 22/27 [00:01<00:00, 21.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.13333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.18333333333333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 25/27 [00:01<00:00, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.09583333333333334\n",
      "0.19166666666666668\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:01<00:00, 18.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(h.keys()):\n",
    "    data = h[key]\n",
    "    t = np.array((data['desired_dist'][()]))\n",
    "    result = ndimage.median_filter(t, size=12)\n",
    "    print((result == 0).mean())\n",
    "    if not (len(data.keys()) >= 5): \n",
    "        print(key, data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5_file4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mh5_file4\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m99c94dc769b5d96e|2018-05-13--20-53-38_29\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m data_new \u001b[38;5;241m=\u001b[39m h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m99c94dc769b5d96e|2018-05-13--20-53-38_29\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5_file4' is not defined"
     ]
    }
   ],
   "source": [
    "data = h5_file4['99c94dc769b5d96e|2018-05-13--20-53-38_29']\n",
    "data_new = h['99c94dc769b5d96e|2018-05-13--20-53-38_29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.keys():\n",
    "    if col in ['angle', 'brake', 'dist', 'gas']: continue\n",
    "    print(key,col)\n",
    "    dt = np.float32 if col != 'image' else int#\n",
    "    dataset_name = col #groups are divided by '/'\n",
    "    a = data[col]\n",
    "    data_new.create_dataset(dataset_name, data = a, compression_opts=6, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object '99c94dc769b5d96e|2018-05-13--20-53-38_29' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_new \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m99c94dc769b5d96e|2018-05-13--20-53-38_29\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m data_new\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/OPT/lib/python3.12/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:241\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object '99c94dc769b5d96e|2018-05-13--20-53-38_29' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "data_new = h['99c94dc769b5d96e|2018-05-13--20-53-38_29']\n",
    "data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in tqdm(keys4):\n",
    "#     data = h5_file4[key]\n",
    "#     if key != '99c94dc769b5d96e|2018-05-13--20-53-38_29': \n",
    "#         continue\n",
    "#     group  = h.create_group(key)\n",
    "#     for col in data.keys():\n",
    "#         if col in ['angle', 'brake', 'dist', 'gas']: continue\n",
    "#         print(key,col)\n",
    "#         dt = np.float32 if col != 'image' else int#\n",
    "#         dataset_name = col #groups are divided by '/'\n",
    "#         a = data[col]\n",
    "#         group.create_dataset(dataset_name, data = a, compression_opts=6, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
